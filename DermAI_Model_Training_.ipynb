{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObdj6RL7bVIcVNZJc4HmIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghad-Odwan/DermAI-Train/blob/main/DermAI_Model_Training_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aM9j1mMPmK1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-BZSwgpySG-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtSwYn7MR8jT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ujA-N-lvmNt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DermAI_AI_Model_Training**"
      ],
      "metadata": {
        "id": "dJqf6nCNSK99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Google Colab notebook has been prepared for the preparation and training of a machine learning model specialized in skin cancer detection.\n",
        "The model performs binary classification of skin lesion images into two categories: Benign and Malignant.\n",
        "\n",
        "\n",
        "### Dataset Source\n",
        "\n",
        "The dataset used in this project was collected from the following sources:\n",
        "(                  _____                )\n",
        "\n",
        "The dataset contains approximately 13,249 benign and 6,211 malignant images, providing a total of around 19,460 samples used for training, validation, and testing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Notebook Structure\n",
        "\n",
        "This notebook is organized into three main sections:\n",
        "\n",
        "**-Data Loading, Verification, and Preprocessing**\n",
        "\n",
        "This section focuses on importing the dataset, verifying its structure, cleaning inconsistencies, and performing Exploratory Data Analysis (EDA).\n",
        "Steps include resizing, normalization, data augmentation, and splitting the dataset into training, validation, and testing subsets.\n",
        "\n",
        "**-Model Training and Evaluation**\n",
        "\n",
        "In this section, a machine learning model is implemented and trained for skin lesion classification.\n",
        "The process includes model configuration, training, and performance evaluation using metrics such as accuracy, precision, recall, and F1-score.\n",
        "Optimization methods are also applied to ensure stable and efficient training.\n",
        "\n",
        "**-Result Interpretation and Visualization**\n",
        "\n",
        "This part is dedicated to analyzing the model’s predictions and interpreting its decision-making process using Grad-CAM and other visualization tools.\n",
        "It highlights how the model distinguishes between benign and malignant lesions, providing insights into reliability and interpretability."
      ],
      "metadata": {
        "id": "AvegU7jsUFOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Note: This notebook represents a core component of the DermAI Graduation Project at Palestine Technical University – Kadoorie.\n",
        "It aims to demonstrate the end-to-end process of building an intelligent, interpretable, and efficient system for skin cancer classification, contributing to early detection and supporting clinical decision-making.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6PuzGd2ij6kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Part One: Dataset Preparation & Preprocessing**"
      ],
      "metadata": {
        "id": "dlNwr0kGmi-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "MtslpsqEsU88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define main dataset path\n",
        "base_dir = \"/content/drive/MyDrive/Dataset/Dataset\"\n",
        "folders = [\"benign\", \"malignant\"]\n",
        "\n",
        "print(\"Base directory:\", base_dir)\n",
        "for folder in folders:\n",
        "    path = os.path.join(base_dir, folder)\n",
        "    print(f\"{folder}: {len(os.listdir(path))} files\")"
      ],
      "metadata": {
        "id": "-VQSvLBtsbCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for problematic images\n",
        "dup_dir = os.path.join(base_dir, \"duplicates_or_corrupted\")\n",
        "os.makedirs(dup_dir, exist_ok=True)\n",
        "print(\"Duplicate/Corrupted folder created at:\", dup_dir)"
      ],
      "metadata": {
        "id": "JJ8eTjKisdq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an image cleaning class\n",
        "class ImageCleaner:\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.folders_to_check = ['benign', 'malignant']\n",
        "        self.problem_folder = self.base_path / 'duplicates_or_corrupted'\n",
        "        self.problem_folder.mkdir(exist_ok=True)\n",
        "        self.stats = {'total_checked': 0, 'corrupted': 0, 'duplicates': 0, 'low_quality': 0, 'healthy': 0}\n",
        "        self.image_hashes = defaultdict(list)\n",
        "\n",
        "    def calculate_hash(self, image_path):\n",
        "        import hashlib\n",
        "        try:\n",
        "            hasher = hashlib.md5()\n",
        "            with open(image_path, 'rb') as f:\n",
        "                buf = f.read()\n",
        "                hasher.update(buf)\n",
        "            return hasher.hexdigest()\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def is_image_corrupted(self, image_path):\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                img.verify()\n",
        "            return False\n",
        "        except:\n",
        "            return True\n",
        "\n",
        "    def check_image_quality(self, image_path, min_width=50, min_height=50):\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                width, height = img.size\n",
        "                if width < min_width or height < min_height:\n",
        "                    return False\n",
        "                if os.path.getsize(image_path) < 1000:\n",
        "                    return False\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def move_to_problem_folder(self, image_path):\n",
        "        try:\n",
        "            dest_subfolder = self.problem_folder / image_path.parent.name\n",
        "            dest_subfolder.mkdir(exist_ok=True)\n",
        "            shutil.move(str(image_path), str(dest_subfolder / image_path.name))\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {image_path.name}: {e}\")\n",
        "\n",
        "    def clean_folder(self, folder_name):\n",
        "        folder_path = self.base_path / folder_name\n",
        "        image_exts = ['.jpg', '.jpeg', '.png']\n",
        "        images = [f for f in folder_path.iterdir() if f.suffix.lower() in image_exts]\n",
        "        print(f\"Cleaning {folder_name} ({len(images)} images)...\")\n",
        "\n",
        "        for img_path in tqdm(images, desc=f\"Checking {folder_name}\"):\n",
        "            self.stats['total_checked'] += 1\n",
        "            if self.is_image_corrupted(img_path):\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['corrupted'] += 1\n",
        "                continue\n",
        "            if not self.check_image_quality(img_path):\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['low_quality'] += 1\n",
        "                continue\n",
        "            img_hash = self.calculate_hash(img_path)\n",
        "            if img_hash in self.image_hashes:\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['duplicates'] += 1\n",
        "            else:\n",
        "                self.image_hashes[img_hash].append(str(img_path))\n",
        "                self.stats['healthy'] += 1\n",
        "\n",
        "    def clean_all(self):\n",
        "        for folder in self.folders_to_check:\n",
        "            self.clean_folder(folder)\n",
        "        print(\"\\nCleaning Summary:\")\n",
        "        for k, v in self.stats.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "\n",
        "# Run the cleaning process\n",
        "cleaner = ImageCleaner(base_dir)\n",
        "cleaner.clean_all()"
      ],
      "metadata": {
        "id": "CO9lRiMgsjp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize class distribution\n",
        "count_benign = len(os.listdir(os.path.join(base_dir, \"benign\")))\n",
        "count_malignant = len(os.listdir(os.path.join(base_dir, \"malignant\")))\n",
        "plt.bar([\"Benign\", \"Malignant\"], [count_benign, count_malignant])\n",
        "plt.title(\"Class Distribution After Cleaning\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Aa5ODGQs6NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize all images to (224x224)\n",
        "IMG_SIZE = (224, 224)\n",
        "for cat in folders:\n",
        "    src_dir = os.path.join(base_dir, cat)\n",
        "    files = os.listdir(src_dir)\n",
        "    for fname in tqdm(files, desc=f\"Resizing {cat}\"):\n",
        "        path = os.path.join(src_dir, fname)\n",
        "        try:\n",
        "            img = cv2.imread(path)\n",
        "            if img is None: continue\n",
        "            resized = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "            cv2.imwrite(path, resized)\n",
        "        except:\n",
        "            continue"
      ],
      "metadata": {
        "id": "yuesJ_ALs8Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset (70% train, 15% val, 15% test)\n",
        "split_dir = \"/content/ai/Dataset_split\"\n",
        "os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "rows = []\n",
        "for label in folders:\n",
        "    path = os.path.join(base_dir, label)\n",
        "    for fname in os.listdir(path):\n",
        "        if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            rows.append({'path': os.path.join(path, fname), 'label': label})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "train_temp, test = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n",
        "train, val = train_test_split(train_temp, test_size=0.1765, stratify=train_temp['label'], random_state=42)\n",
        "\n",
        "for subset in ['train', 'val', 'test']:\n",
        "    for label in folders:\n",
        "        os.makedirs(os.path.join(split_dir, subset, label), exist_ok=True)\n",
        "\n",
        "def copy_images(df_subset, subset_name):\n",
        "    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=f\"Copying {subset_name}\"):\n",
        "        dest = os.path.join(split_dir, subset_name, row['label'], os.path.basename(row['path']))\n",
        "        shutil.copy2(row['path'], dest)\n",
        "\n",
        "copy_images(train, \"train\")\n",
        "copy_images(val, \"val\")\n",
        "copy_images(test, \"test\")\n",
        "\n",
        "print(f\"\\nDataset split completed successfully!\")\n",
        "print(f\"Train: {len(train)} | Val: {len(val)} | Test: {len(test)}\")"
      ],
      "metadata": {
        "id": "dXqP3MmYtJYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "####  Build DataFrame & Quick Integrity Check"
      ],
      "metadata": {
        "id": "2BrWAksE227X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of image files in each class folder (benign and malignant)\n",
        "# This function walks through all subdirectories and counts only valid image files.\n",
        "import os, sys, traceback\n",
        "base_path = \"/content/drive/MyDrive/Dataset/Dataset\"\n",
        "\n",
        "def count_images_in_folder(folder):\n",
        "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
        "    cnt = 0\n",
        "    for root, dirs, files in os.walk(folder):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(exts):\n",
        "                cnt += 1\n",
        "    return cnt\n",
        "\n",
        "for cls in ['benign','malignant']:\n",
        "    p = os.path.join(base_path, cls)\n",
        "    if not os.path.exists(p):\n",
        "        print(f\" WARNING: folder not found: {p}\")\n",
        "    else:\n",
        "        print(f\"{cls}: {count_images_in_folder(p):,} images\")"
      ],
      "metadata": {
        "id": "ARx5owga3By7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "# build dataframe (paths + labels)\n",
        "rows=[]\n",
        "exts = ('.jpg','.jpeg','.png','.bmp')\n",
        "for cls in ['benign','malignant']:\n",
        "    folder = os.path.join(base_path, cls)\n",
        "    if not os.path.exists(folder):\n",
        "        continue\n",
        "    for root, dirs, files in os.walk(folder):\n",
        "        for fname in files:\n",
        "            if fname.lower().endswith(exts):\n",
        "                rows.append({'path': os.path.join(root, fname), 'label': cls})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df['label_idx'] = df['label'].map({'benign':0, 'malignant':1})\n",
        "print(\"Total samples:\", len(df))\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "id": "7z8QjeKr30K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  quick corrupted-files check (lightweight, may take time if dataset big)\n",
        "#  try to open the first N images from each class to detect obvious corruption\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "def check_samples(df, n_per_class=20):\n",
        "    corrupted = []\n",
        "    for cls in df['label'].unique():\n",
        "        paths = df[df['label']==cls]['path'].tolist()\n",
        "        sample_paths = random.sample(paths, min(n_per_class, len(paths)))\n",
        "        for p in sample_paths:\n",
        "            try:\n",
        "                img = Image.open(p)\n",
        "                img.verify()\n",
        "            except Exception as e:\n",
        "                corrupted.append((p, str(e)))\n",
        "    return corrupted\n",
        "\n",
        "corrupted_examples = check_samples(df, n_per_class=30)\n",
        "if corrupted_examples:\n",
        "    print(\" Found corrupted or unreadable sample(s):\", len(corrupted_examples))\n",
        "    for p,err in corrupted_examples[:5]:\n",
        "        print(\"-\", p, \"=>\", err)\n",
        "else:\n",
        "    print(\" Quick corrupted-sample check passed successfully (no issues in sampled files).\")"
      ],
      "metadata": {
        "id": "dho8-d694E6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save metadata CSV\n",
        "out_csv = \"/content/drive/MyDrive/ai/data/df_metadata.csv\"\n",
        "os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(\" Metadata saved to:\", out_csv)"
      ],
      "metadata": {
        "id": "BX7JGCIv4dht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.head(10))\n",
        "print(\"\\nCounts (sanity):\")\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "id": "4X8S6Okp6JrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test reading data\n",
        "# Randomly load and display one sample image from the dataset\n",
        "# to verify that image paths are correct and preprocessing worked properly.\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample = random.choice(df['path'].tolist())\n",
        "img = image.load_img(sample, target_size=(224,224))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(sample.split('/')[-2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UY4uoFC17E4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Exo-cHl6Ce_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part Two: Model Training and Evaluation**"
      ],
      "metadata": {
        "id": "Mj7gChfDChNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "# Parameters\n",
        "# Defines image size, batch size, epochs, and model saving directory\n",
        "DRIVE_BASE = \"/content/drive/MyDrive\"\n",
        "DF_PATH = os.path.join(DRIVE_BASE, \"ai/data/df_metadata.csv\")\n",
        "MODELS_DIR = os.path.join(DRIVE_BASE, \"DermAI_models_resnet\")\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "N_FOLDS = 3\n",
        "RANDOM_STATE = 42\n",
        "VERBOSE = 1"
      ],
      "metadata": {
        "id": "d8hnWIppDKDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)"
      ],
      "metadata": {
        "id": "I3NIU_7RESJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read & Inspect Metadata\n",
        "meta_csv = \"/content/drive/MyDrive/ai/data/df_metadata.csv\"\n",
        "df = pd.read_csv(meta_csv)\n",
        "\n",
        "print(\"Loaded df:\", meta_csv)\n",
        "print(\"Total samples:\", len(df))\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "display(df.head(10))\n",
        "\n",
        "# Prepare X and y for training\n",
        "X = df['path'].values\n",
        "y = df['label_idx'].values"
      ],
      "metadata": {
        "id": "_emNvBXnEcr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generators"
      ],
      "metadata": {
        "id": "p5N0CPqKE9Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare image generators for training and validation.\n",
        "# Training generator applies data augmentation to improve model generalization,\n",
        "# while validation generator only rescales pixel values.\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    brightness_range=(0.8,1.2),\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "nKyxJfbVFAhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IYJjSx-Ky7yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"Mixed precision enabled.\")"
      ],
      "metadata": {
        "id": "M7tY-G2-y_dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OsmmzHpuy93r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building"
      ],
      "metadata": {
        "id": "mMDmGUZ8Fy7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_resnet(input_shape=(224,224,3), n_classes=2):\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable = False\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(n_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base.input, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "LkR6aL9QFxkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HgzHe6mtGMfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a copy of data locally\n",
        "import os, shutil, time\n",
        "DRIVE_SPLIT = \"/content/ai/Dataset_split\"\n",
        "LOCAL_SPLIT = \"/content/local_dataset_split\"\n",
        "\n",
        "if not os.path.exists(LOCAL_SPLIT):\n",
        "    print(\"Copying dataset to local disk (may take several minutes)...\")\n",
        "    start=time.time()\n",
        "    shutil.copytree(DRIVE_SPLIT, LOCAL_SPLIT)\n",
        "    print(\"Copy finished in %.1f s\" % (time.time()-start))\n",
        "else:\n",
        "    print(\"Local dataset already exists:\", LOCAL_SPLIT)"
      ],
      "metadata": {
        "id": "F-Ob-6ALdOCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build local dataframe from the copied split\n",
        "import pandas as pd, os\n",
        "rows = []\n",
        "for subset in ['train','val','test']:\n",
        "    for cls in ['benign','malignant']:\n",
        "        p = os.path.join(LOCAL_SPLIT, subset, cls)\n",
        "        if not os.path.exists(p): continue\n",
        "        for f in os.listdir(p):\n",
        "            if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                rows.append({\n",
        "                    'path': os.path.join(p,f),\n",
        "                    'label': cls,\n",
        "                    'label_idx': 0 if cls=='benign' else 1,\n",
        "                    'subset': subset\n",
        "                })\n",
        "df_local = pd.DataFrame(rows).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Local dataset summary:\\n\", df_local.groupby(['subset','label']).size())"
      ],
      "metadata": {
        "id": "BDQ4RZjJw0OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nohup bash -c 'while true; do date; nvidia-smi; echo \"--------------------------------------------------\"; sleep 300; done' > /content/gpu_monitor.log 2>&1 &\n",
        "echo \"GPU monitor started — logging to /content/gpu_monitor.log\""
      ],
      "metadata": {
        "id": "O1TgK7OgwcVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xiAGg0FDdM15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Functions for Dataset Loading & Preprocessing\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def decode_and_resize(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = img / 255.0\n",
        "    return img, label\n",
        "\n",
        "def make_dataset(paths, labels, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(2048)\n",
        "    ds = ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "jCiRJ5_UxN9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Fold Preparation and Speed Test\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np, time, math\n",
        "\n",
        "# Prepare CV on train, val\n",
        "df_cv = df_local[df_local['subset'] != 'test'].reset_index(drop=True)\n",
        "X = df_cv['path'].values\n",
        "y = df_cv['label_idx'].values\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "train_idx, val_idx = next(skf.split(X, y))  # خذ fold1 فقط\n",
        "train_paths, train_labels = X[train_idx], y[train_idx]\n",
        "val_paths, val_labels = X[val_idx], y[val_idx]\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_paths, train_labels, shuffle=True)\n",
        "val_ds = make_dataset(val_paths, val_labels, shuffle=False)\n",
        "\n",
        "# build small frozen ResNet for test speed\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_resnet_small():\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0],IMG_SIZE[1],3))\n",
        "    base.trainable = False\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(2, activation='softmax', dtype='float32')(x)\n",
        "    model = Model(inputs=base.input, outputs=out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_resnet_small()\n",
        "\n",
        "# measure speed (K batches)\n",
        "K = 50\n",
        "it = iter(train_ds)\n",
        "# warm up\n",
        "_ = next(it)\n",
        "start = time.time()\n",
        "for i in range(K):\n",
        "    try:\n",
        "        xb, yb = next(it)\n",
        "    except StopIteration:\n",
        "        it = iter(train_ds); xb, yb = next(it)\n",
        "    model.train_on_batch(xb, yb)\n",
        "elapsed = time.time() - start\n",
        "sec_per_step = elapsed / K\n",
        "steps_per_epoch = math.ceil(len(train_paths)/BATCH_SIZE)\n",
        "print(\"sec_per_step:\", sec_per_step)\n",
        "print(\"Estimated epoch time (min):\", (steps_per_epoch*sec_per_step)/60)"
      ],
      "metadata": {
        "id": "SDEhqrlTkrnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H9f0GOOskugN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # For each fold:\n",
        "#   • Split data into training and validation subsets while preserving class balance.\n",
        "#   • Build and train a ResNet50 model on the training subset.\n",
        "#   • Evaluate model performance on the validation subset (Accuracy, Precision, Recall, F1).\n",
        "#   • Save best model weights and record per-fold metrics.\n",
        "# Results from all folds are stored in 'fold_metrics' for later summary and analysis.\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold_metrics = []\n",
        "fold_no = 1\n",
        "\n",
        "for train_idx, val_idx in skf.split(X, y):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\" Starting Fold {fold_no}/3 — Train:{len(train_idx)} | Val:{len(val_idx)}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    train_df = pd.DataFrame({'path': X[train_idx], 'label_idx': y[train_idx]})\n",
        "    val_df   = pd.DataFrame({'path': X[val_idx],   'label_idx': y[val_idx]})\n",
        "\n",
        "    classes = np.unique(train_df['label_idx'])\n",
        "    cw = compute_class_weight('balanced', classes=classes, y=train_df['label_idx'])\n",
        "    class_weight = {int(c): w for c,w in zip(classes, cw)}\n",
        "\n",
        "    train_gen = train_datagen.flow_from_dataframe(train_df, x_col='path', y_col='label_idx',\n",
        "                                                  target_size=IMG_SIZE, class_mode='raw',\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_gen = val_datagen.flow_from_dataframe(val_df, x_col='path', y_col='label_idx',\n",
        "                                              target_size=IMG_SIZE, class_mode='raw',\n",
        "                                              batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = build_resnet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), n_classes=2)\n",
        "    ckpt_path = os.path.join(MODELS_DIR, f\"best_resnet_fold{fold_no}.keras\")\n",
        "\n",
        "    csv_log_path = os.path.join(MODELS_DIR, f\"training_log_fold{fold_no}.csv\")\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
        "        ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "        CSVLogger(csv_log_path, append=True)\n",
        "    ]\n",
        "\n",
        "    steps_per_epoch = math.ceil(len(train_df) / BATCH_SIZE)\n",
        "    val_steps = math.ceil(len(val_df) / BATCH_SIZE)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=val_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=val_steps,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callbacks,\n",
        "        verbose=VERBOSE\n",
        "    )\n",
        "\n",
        "    # Evaluate model performance for this fold\n",
        "    val_gen.reset()\n",
        "    preds_prob = model.predict(val_gen, steps=val_steps, verbose=VERBOSE)\n",
        "    preds = np.argmax(preds_prob, axis=1)\n",
        "    true = val_df['label_idx'].values[:len(preds)]\n",
        "\n",
        "    acc = accuracy_score(true, preds)\n",
        "    prec = precision_score(true, preds, zero_division=0)\n",
        "    rec = recall_score(true, preds, zero_division=0)\n",
        "    f1 = f1_score(true, preds, zero_division=0)\n",
        "\n",
        "    print(f\"Fold {fold_no} -> acc:{acc:.4f}, prec:{prec:.4f}, rec:{rec:.4f}, f1:{f1:.4f}\")\n",
        "    print(classification_report(true, preds, target_names=['benign','malignant']))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(true, preds))\n",
        "\n",
        "    fold_metrics.append({'fold':fold_no, 'accuracy':acc,'precision':prec,'recall':rec,'f1':f1})\n",
        "    fold_no += 1\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "bJidgjQUGN7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8bf550-30e6-4ceb-9168-e5bbe8f62f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            " Starting Fold 1/3 — Train:11052 | Val:5527\n",
            "============================================================\n",
            "Found 11052 validated image filenames.\n",
            "Found 5527 validated image filenames.\n",
            "Epoch 1/20\n",
            "\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.5227 - loss: 0.8637"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the resultes\n",
        "metrics_df = pd.DataFrame(fold_metrics)\n",
        "metrics_csv = os.path.join(MODELS_DIR, \"resnet3fold_metrics.csv\")\n",
        "metrics_df.to_csv(metrics_csv, index=False)\n",
        "print(\"\\nSaved metrics to:\", metrics_csv)\n",
        "print(\"\\nPer-fold metrics:\\n\", metrics_df)\n",
        "print(\"\\nMean metrics:\\n\", metrics_df[['accuracy','precision','recall','f1']].mean())\n",
        "print(\"\\nStd metrics:\\n\", metrics_df[['accuracy','precision','recall','f1']].std())"
      ],
      "metadata": {
        "id": "5V_auEesGuXE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}