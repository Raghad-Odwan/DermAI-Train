{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghad-Odwan/DermAI-Train/blob/main/DermAI_Model_Training_%20NewEitd%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM9j1mMPmK1C"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student model training session\n"
      ],
      "metadata": {
        "id": "LW3Rs0qOOl8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-BZSwgpySG-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c92521c-f961-4a7d-8f5a-9aa7054b32c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/checkpoints"
      ],
      "metadata": {
        "id": "KnOEhTE2PYL8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WtSwYn7MR8jT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5a2254-11df-44ed-b3c7-c2ed7655e7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "    console.log(\"Preventing Colab timeout\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click();\n",
        "}\n",
        "setInterval(ClickConnect, 60000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VCfgt1ggPLpH",
        "outputId": "d6bab1f3-2911-4808-e4f9-a681d20c8eed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "    console.log(\"Preventing Colab timeout\");\n",
              "    document.querySelector(\"colab-toolbar-button#connect\").click();\n",
              "}\n",
              "setInterval(ClickConnect, 60000)\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujA-N-lvmNt_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJqf6nCNSK99"
      },
      "source": [
        "# **DermAI_AI_Model_Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvegU7jsUFOa"
      },
      "source": [
        "This Google Colab notebook has been prepared for the preparation and training of a machine learning model specialized in skin cancer detection.\n",
        "The model performs binary classification of skin lesion images into two categories: Benign and Malignant.\n",
        "\n",
        "\n",
        "### Dataset Source\n",
        "\n",
        "The dataset used in this project was collected from the following sources:\n",
        "(                  _____                )\n",
        "\n",
        "The dataset contains approximately 13,249 benign and 6,211 malignant images, providing a total of around 19,460 samples used for training, validation, and testing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Notebook Structure\n",
        "\n",
        "This notebook is organized into three main sections:\n",
        "\n",
        "**-Data Loading, Verification, and Preprocessing**\n",
        "\n",
        "This section focuses on importing the dataset, verifying its structure, cleaning inconsistencies, and performing Exploratory Data Analysis (EDA).\n",
        "Steps include resizing, normalization, data augmentation, and splitting the dataset into training, validation, and testing subsets.\n",
        "\n",
        "**-Model Training and Evaluation**\n",
        "\n",
        "In this section, a machine learning model is implemented and trained for skin lesion classification.\n",
        "The process includes model configuration, training, and performance evaluation using metrics such as accuracy, precision, recall, and F1-score.\n",
        "Optimization methods are also applied to ensure stable and efficient training.\n",
        "\n",
        "**-Result Interpretation and Visualization**\n",
        "\n",
        "This part is dedicated to analyzing the model’s predictions and interpreting its decision-making process using Grad-CAM and other visualization tools.\n",
        "It highlights how the model distinguishes between benign and malignant lesions, providing insights into reliability and interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PuzGd2ij6kK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Note: This notebook represents a core component of the DermAI Graduation Project at Palestine Technical University – Kadoorie.\n",
        "It aims to demonstrate the end-to-end process of building an intelligent, interpretable, and efficient system for skin cancer classification, contributing to early detection and supporting clinical decision-making.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlNwr0kGmi-h"
      },
      "source": [
        "## 1. **Part One: Dataset Preparation & Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MtslpsqEsU88"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-VQSvLBtsbCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ed504d-5c78-4732-a239-f5ef8307c0b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base directory: /content/drive/MyDrive/Dataset/Dataset\n",
            "benign: 13294 files\n",
            "malignant: 6211 files\n"
          ]
        }
      ],
      "source": [
        "# Define main dataset path\n",
        "base_dir = \"/content/drive/MyDrive/Dataset/Dataset\"\n",
        "folders = [\"benign\", \"malignant\"]\n",
        "\n",
        "print(\"Base directory:\", base_dir)\n",
        "for folder in folders:\n",
        "    path = os.path.join(base_dir, folder)\n",
        "    print(f\"{folder}: {len(os.listdir(path))} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JJ8eTjKisdq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412589d0-79c0-4cf7-d472-6657fe2f8e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate/Corrupted folder created at: /content/drive/MyDrive/Dataset/Dataset/duplicates_or_corrupted\n"
          ]
        }
      ],
      "source": [
        "# Create a folder for problematic images\n",
        "dup_dir = os.path.join(base_dir, \"duplicates_or_corrupted\")\n",
        "os.makedirs(dup_dir, exist_ok=True)\n",
        "print(\"Duplicate/Corrupted folder created at:\", dup_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO9lRiMgsjp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c05aa4f-8918-462b-c8fe-4868707353bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning benign (13294 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking benign:   1%|          | 134/13294 [02:02<58:41:27, 16.06s/it]"
          ]
        }
      ],
      "source": [
        "# Define an image cleaning class\n",
        "class ImageCleaner:\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.folders_to_check = ['benign', 'malignant']\n",
        "        self.problem_folder = self.base_path / 'duplicates_or_corrupted'\n",
        "        self.problem_folder.mkdir(exist_ok=True)\n",
        "        self.stats = {'total_checked': 0, 'corrupted': 0, 'duplicates': 0, 'low_quality': 0, 'healthy': 0}\n",
        "        self.image_hashes = defaultdict(list)\n",
        "\n",
        "    def calculate_hash(self, image_path):\n",
        "        import hashlib\n",
        "        try:\n",
        "            hasher = hashlib.md5()\n",
        "            with open(image_path, 'rb') as f:\n",
        "                buf = f.read()\n",
        "                hasher.update(buf)\n",
        "            return hasher.hexdigest()\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def is_image_corrupted(self, image_path):\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                img.verify()\n",
        "            return False\n",
        "        except:\n",
        "            return True\n",
        "\n",
        "    def check_image_quality(self, image_path, min_width=50, min_height=50):\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                width, height = img.size\n",
        "                if width < min_width or height < min_height:\n",
        "                    return False\n",
        "                if os.path.getsize(image_path) < 1000:\n",
        "                    return False\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def move_to_problem_folder(self, image_path):\n",
        "        try:\n",
        "            dest_subfolder = self.problem_folder / image_path.parent.name\n",
        "            dest_subfolder.mkdir(exist_ok=True)\n",
        "            shutil.move(str(image_path), str(dest_subfolder / image_path.name))\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {image_path.name}: {e}\")\n",
        "\n",
        "    def clean_folder(self, folder_name):\n",
        "        folder_path = self.base_path / folder_name\n",
        "        image_exts = ['.jpg', '.jpeg', '.png']\n",
        "        images = [f for f in folder_path.iterdir() if f.suffix.lower() in image_exts]\n",
        "        print(f\"Cleaning {folder_name} ({len(images)} images)...\")\n",
        "\n",
        "        for img_path in tqdm(images, desc=f\"Checking {folder_name}\"):\n",
        "            self.stats['total_checked'] += 1\n",
        "            if self.is_image_corrupted(img_path):\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['corrupted'] += 1\n",
        "                continue\n",
        "            if not self.check_image_quality(img_path):\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['low_quality'] += 1\n",
        "                continue\n",
        "            img_hash = self.calculate_hash(img_path)\n",
        "            if img_hash in self.image_hashes:\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['duplicates'] += 1\n",
        "            else:\n",
        "                self.image_hashes[img_hash].append(str(img_path))\n",
        "                self.stats['healthy'] += 1\n",
        "\n",
        "    def clean_all(self):\n",
        "        for folder in self.folders_to_check:\n",
        "            self.clean_folder(folder)\n",
        "        print(\"\\nCleaning Summary:\")\n",
        "        for k, v in self.stats.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "\n",
        "# Run the cleaning process\n",
        "cleaner = ImageCleaner(base_dir)\n",
        "cleaner.clean_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Aa5ODGQs6NI"
      },
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "count_benign = len(os.listdir(os.path.join(base_dir, \"benign\")))\n",
        "count_malignant = len(os.listdir(os.path.join(base_dir, \"malignant\")))\n",
        "plt.bar([\"Benign\", \"Malignant\"], [count_benign, count_malignant])\n",
        "plt.title(\"Class Distribution After Cleaning\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuesJ_ALs8Cq"
      },
      "outputs": [],
      "source": [
        "# Resize all images to (224x224)\n",
        "IMG_SIZE = (224, 224)\n",
        "for cat in folders:\n",
        "    src_dir = os.path.join(base_dir, cat)\n",
        "    files = os.listdir(src_dir)\n",
        "    for fname in tqdm(files, desc=f\"Resizing {cat}\"):\n",
        "        path = os.path.join(src_dir, fname)\n",
        "        try:\n",
        "            img = cv2.imread(path)\n",
        "            if img is None: continue\n",
        "            resized = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "            cv2.imwrite(path, resized)\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXqP3MmYtJYu"
      },
      "outputs": [],
      "source": [
        "# Split dataset (70% train, 15% val, 15% test)\n",
        "split_dir = \"/content/ai/Dataset_split\"\n",
        "os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "rows = []\n",
        "for label in folders:\n",
        "    path = os.path.join(base_dir, label)\n",
        "    for fname in os.listdir(path):\n",
        "        if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            rows.append({'path': os.path.join(path, fname), 'label': label})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "train_temp, test = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n",
        "train, val = train_test_split(train_temp, test_size=0.1765, stratify=train_temp['label'], random_state=42)\n",
        "\n",
        "for subset in ['train', 'val', 'test']:\n",
        "    for label in folders:\n",
        "        os.makedirs(os.path.join(split_dir, subset, label), exist_ok=True)\n",
        "\n",
        "def copy_images(df_subset, subset_name):\n",
        "    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=f\"Copying {subset_name}\"):\n",
        "        dest = os.path.join(split_dir, subset_name, row['label'], os.path.basename(row['path']))\n",
        "        shutil.copy2(row['path'], dest)\n",
        "\n",
        "copy_images(train, \"train\")\n",
        "copy_images(val, \"val\")\n",
        "copy_images(test, \"test\")\n",
        "\n",
        "print(f\"\\nDataset split completed successfully!\")\n",
        "print(f\"Train: {len(train)} | Val: {len(val)} | Test: {len(test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BrWAksE227X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "####  Build DataFrame & Quick Integrity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARx5owga3By7"
      },
      "outputs": [],
      "source": [
        "# Count the number of image files in each class folder (benign and malignant)\n",
        "# This function walks through all subdirectories and counts only valid image files.\n",
        "import os, sys, traceback\n",
        "base_path = \"/content/drive/MyDrive/Dataset/Dataset\"\n",
        "\n",
        "def count_images_in_folder(folder):\n",
        "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
        "    cnt = 0\n",
        "    for root, dirs, files in os.walk(folder):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(exts):\n",
        "                cnt += 1\n",
        "    return cnt\n",
        "\n",
        "for cls in ['benign','malignant']:\n",
        "    p = os.path.join(base_path, cls)\n",
        "    if not os.path.exists(p):\n",
        "        print(f\" WARNING: folder not found: {p}\")\n",
        "    else:\n",
        "        print(f\"{cls}: {count_images_in_folder(p):,} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z8QjeKr30K-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "# build dataframe (paths + labels)\n",
        "rows=[]\n",
        "exts = ('.jpg','.jpeg','.png','.bmp')\n",
        "for cls in ['benign','malignant']:\n",
        "    folder = os.path.join(base_path, cls)\n",
        "    if not os.path.exists(folder):\n",
        "        continue\n",
        "    for root, dirs, files in os.walk(folder):\n",
        "        for fname in files:\n",
        "            if fname.lower().endswith(exts):\n",
        "                rows.append({'path': os.path.join(root, fname), 'label': cls})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df['label_idx'] = df['label'].map({'benign':0, 'malignant':1})\n",
        "print(\"Total samples:\", len(df))\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dho8-d694E6j"
      },
      "outputs": [],
      "source": [
        "#  quick corrupted-files check (lightweight, may take time if dataset big)\n",
        "#  try to open the first N images from each class to detect obvious corruption\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "def check_samples(df, n_per_class=20):\n",
        "    corrupted = []\n",
        "    for cls in df['label'].unique():\n",
        "        paths = df[df['label']==cls]['path'].tolist()\n",
        "        sample_paths = random.sample(paths, min(n_per_class, len(paths)))\n",
        "        for p in sample_paths:\n",
        "            try:\n",
        "                img = Image.open(p)\n",
        "                img.verify()\n",
        "            except Exception as e:\n",
        "                corrupted.append((p, str(e)))\n",
        "    return corrupted\n",
        "\n",
        "corrupted_examples = check_samples(df, n_per_class=30)\n",
        "if corrupted_examples:\n",
        "    print(\" Found corrupted or unreadable sample(s):\", len(corrupted_examples))\n",
        "    for p,err in corrupted_examples[:5]:\n",
        "        print(\"-\", p, \"=>\", err)\n",
        "else:\n",
        "    print(\" Quick corrupted-sample check passed successfully (no issues in sampled files).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX7JGCIv4dht"
      },
      "outputs": [],
      "source": [
        "# Save metadata CSV\n",
        "out_csv = \"/content/drive/MyDrive/ai/data/df_metadata.csv\"\n",
        "os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(\" Metadata saved to:\", out_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X8S6Okp6JrA"
      },
      "outputs": [],
      "source": [
        "display(df.head(10))\n",
        "print(\"\\nCounts (sanity):\")\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY4uoFC17E4w"
      },
      "outputs": [],
      "source": [
        "# Quick test reading data\n",
        "# Randomly load and display one sample image from the dataset\n",
        "# to verify that image paths are correct and preprocessing worked properly.\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample = random.choice(df['path'].tolist())\n",
        "img = image.load_img(sample, target_size=(224,224))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(sample.split('/')[-2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exo-cHl6Ce_n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj7gChfDChNO"
      },
      "source": [
        "## **Part Two: Model Training and Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8hnWIppDKDF"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "# Parameters\n",
        "# Defines image size, batch size, epochs, and model saving directory\n",
        "DRIVE_BASE = \"/content/drive/MyDrive\"\n",
        "DF_PATH = os.path.join(DRIVE_BASE, \"ai/data/df_metadata.csv\")\n",
        "MODELS_DIR = os.path.join(DRIVE_BASE, \"DermAI_models_resnet\")\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 15\n",
        "N_FOLDS = 3\n",
        "RANDOM_STATE = 42\n",
        "VERBOSE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3NIU_7RESJF"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_emNvBXnEcr0"
      },
      "outputs": [],
      "source": [
        "# Read & Inspect Metadata\n",
        "meta_csv = \"/content/drive/MyDrive/ai/data/df_metadata.csv\"\n",
        "df = pd.read_csv(meta_csv)\n",
        "\n",
        "print(\"Loaded df:\", meta_csv)\n",
        "print(\"Total samples:\", len(df))\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "display(df.head(10))\n",
        "\n",
        "# Prepare X and y for training\n",
        "X = df['path'].values\n",
        "y = df['label_idx'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5N0CPqKE9Ms"
      },
      "source": [
        "Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKyxJfbVFAhQ"
      },
      "outputs": [],
      "source": [
        "# Prepare image generators for training and validation.\n",
        "# Training generator applies data augmentation to improve model generalization,\n",
        "# while validation generator only rescales pixel values.\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJjSx-Ky7yh"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7tY-G2-y_dn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"Mixed precision enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsmmzHpuy93r"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMDmGUZ8Fy7q"
      },
      "source": [
        "Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkR6aL9QFxkH"
      },
      "outputs": [],
      "source": [
        "'''def build_resnet(input_shape=(224,224,3), n_classes=2):\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable = False\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(n_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base.input, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model'''\n",
        "    # Build ResNet with a single-sigmoid output (binary)\n",
        "# Build binary ResNet50 with fine-tuning\n",
        "def build_resnet_binary(input_shape=(224,224,3)):\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Unfreeze last 40 layers for fine-tuning\n",
        "    for layer in base.layers[:-40]:\n",
        "        layer.trainable = False\n",
        "    for layer in base.layers[-40:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base.input, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-5),  # smaller LR for fine-tuning\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgzHe6mtGMfF"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Ob-6ALdOCM"
      },
      "outputs": [],
      "source": [
        "# create a copy of data locally\n",
        "import os, shutil, time\n",
        "DRIVE_SPLIT = \"/content/ai/Dataset_split\"\n",
        "LOCAL_SPLIT = \"/content/local_dataset_split\"\n",
        "\n",
        "if not os.path.exists(LOCAL_SPLIT):\n",
        "    print(\"Copying dataset to local disk (may take several minutes)...\")\n",
        "    start=time.time()\n",
        "    shutil.copytree(DRIVE_SPLIT, LOCAL_SPLIT)\n",
        "    print(\"Copy finished in %.1f s\" % (time.time()-start))\n",
        "else:\n",
        "    print(\"Local dataset already exists:\", LOCAL_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDQ4RZjJw0OR"
      },
      "outputs": [],
      "source": [
        "# Build local dataframe from the copied split\n",
        "import pandas as pd, os\n",
        "rows = []\n",
        "for subset in ['train','val','test']:\n",
        "    for cls in ['benign','malignant']:\n",
        "        p = os.path.join(LOCAL_SPLIT, subset, cls)\n",
        "        if not os.path.exists(p): continue\n",
        "        for f in os.listdir(p):\n",
        "            if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                rows.append({\n",
        "                    'path': os.path.join(p,f),\n",
        "                    'label': cls,\n",
        "                    'label_idx': 0 if cls=='benign' else 1,\n",
        "                    'subset': subset\n",
        "                })\n",
        "df_local = pd.DataFrame(rows).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Local dataset summary:\\n\", df_local.groupby(['subset','label']).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1TgK7OgwcVv"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "nohup bash -c 'while true; do date; nvidia-smi; echo \"--------------------------------------------------\"; sleep 300; done' > /content/gpu_monitor.log 2>&1 &\n",
        "echo \"GPU monitor started — logging to /content/gpu_monitor.log\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiAGg0FDdM15"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCiRJ5_UxN9Y"
      },
      "outputs": [],
      "source": [
        "# Utility Functions for Dataset Loading & Preprocessing\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def decode_and_resize(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = img / 255.0\n",
        "    return img, label\n",
        "\n",
        "def make_dataset(paths, labels, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(2048)\n",
        "    ds = ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDEhqrlTkrnN"
      },
      "outputs": [],
      "source": [
        "# Start Fold Preparation and Speed Test\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np, time, math\n",
        "\n",
        "# Prepare CV on train, val\n",
        "df_cv = df_local[df_local['subset'] != 'test'].reset_index(drop=True)\n",
        "X = df_cv['path'].values\n",
        "y = df_cv['label_idx'].values\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "train_idx, val_idx = next(skf.split(X, y))  # خذ fold1 فقط\n",
        "train_paths, train_labels = X[train_idx], y[train_idx]\n",
        "val_paths, val_labels = X[val_idx], y[val_idx]\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_paths, train_labels, shuffle=True)\n",
        "val_ds = make_dataset(val_paths, val_labels, shuffle=False)\n",
        "\n",
        "# build small frozen ResNet for test speed\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_resnet_small():\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0],IMG_SIZE[1],3))\n",
        "    base.trainable = False\n",
        "    x = base.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(2, activation='softmax', dtype='float32')(x)\n",
        "    model = Model(inputs=base.input, outputs=out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_resnet_small()\n",
        "\n",
        "# measure speed (K batches)\n",
        "K = 50\n",
        "it = iter(train_ds)\n",
        "# warm up\n",
        "_ = next(it)\n",
        "start = time.time()\n",
        "for i in range(K):\n",
        "    try:\n",
        "        xb, yb = next(it)\n",
        "    except StopIteration:\n",
        "        it = iter(train_ds); xb, yb = next(it)\n",
        "    model.train_on_batch(xb, yb)\n",
        "elapsed = time.time() - start\n",
        "sec_per_step = elapsed / K\n",
        "steps_per_epoch = math.ceil(len(train_paths)/BATCH_SIZE)\n",
        "print(\"sec_per_step:\", sec_per_step)\n",
        "print(\"Estimated epoch time (min):\", (steps_per_epoch*sec_per_step)/60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wuNYik4jNowX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "epO8C1qSNqN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DIAGNOSTIC: check label distribution and generator mapping\n",
        "import numpy as np, math, os\n",
        "\n",
        "print(\"df_local summary (first 10 rows):\")\n",
        "display(df_local.head(10))\n",
        "print(\"\\nGlobal df label counts:\\n\", df['label'].value_counts())\n",
        "print(\"\\nLocal df counts by subset & label:\\n\", df_local.groupby(['subset','label']).size())\n",
        "\n",
        "# If train_gen/val_gen exist, inspect them\n",
        "if 'train_gen' in globals():\n",
        "    print(\"\\ntrain_gen.class_indices:\", getattr(train_gen, 'class_indices', None))\n",
        "    if hasattr(train_gen, 'classes'):\n",
        "        u, c = np.unique(train_gen.classes, return_counts=True)\n",
        "        print(\"train_gen.classes counts:\", dict(zip(u,c)))\n",
        "if 'val_gen' in globals():\n",
        "    print(\"\\nval_gen.class_indices:\", getattr(val_gen, 'class_indices', None))\n",
        "    if hasattr(val_gen, 'classes'):\n",
        "        u, c = np.unique(val_gen.classes, return_counts=True)\n",
        "        print(\"val_gen.classes counts:\", dict(zip(u,c)))\n",
        "\n",
        "# Quick direct check using val_paths/val_labels if available\n",
        "if 'val_paths' in globals() and 'val_labels' in globals():\n",
        "    import collections\n",
        "    print(\"\\nval_labels distribution (array):\", dict(collections.Counter(val_labels)))\n",
        "else:\n",
        "    print(\"\\nval_paths/val_labels not found — skip array check.\")\n",
        "\n",
        "# Sample model prediction test (works if `model` exists in memory)\n",
        "if 'model' in globals() and ('val_gen' in globals() or ('val_paths' in globals() and 'val_labels' in globals())):\n",
        "    print(\"\\nRunning sample prediction check (first batch)...\")\n",
        "    try:\n",
        "        if 'val_gen' in globals():\n",
        "            val_gen.batch_size = min(16, val_gen.batch_size)\n",
        "            val_gen.reset()\n",
        "            xb, yb = next(val_gen)\n",
        "            probs = model.predict(xb, verbose=0)\n",
        "            if probs.ndim == 2 and probs.shape[1] == 2:\n",
        "                preds = np.argmax(probs, axis=1)\n",
        "            else:\n",
        "                preds = (probs.ravel() > 0.5).astype(int)\n",
        "            print(\"sample true unique:\", np.unique(yb, return_counts=True))\n",
        "            print(\"sample preds unique:\", np.unique(preds, return_counts=True))\n",
        "            print(\"sample probs (first 6):\", probs[:6])\n",
        "        else:\n",
        "            # load 16 imgs from val_paths\n",
        "            import tensorflow as tf\n",
        "            def load_img(p):\n",
        "                img = tf.io.read_file(p)\n",
        "                img = tf.image.decode_jpeg(img, channels=3)\n",
        "                img = tf.image.resize(img, (224,224))\n",
        "                return (img/255.0).numpy()\n",
        "            xb = np.stack([load_img(p) for p in val_paths[:16]])\n",
        "            probs = model.predict(xb)\n",
        "            print(\"sample probs (first 6):\", probs[:6])\n",
        "    except Exception as e:\n",
        "        print(\"Prediction test failed:\", e)\n",
        "else:\n",
        "    print(\"\\nModel or validation data not available in memory for sample prediction.\")\n"
      ],
      "metadata": {
        "id": "mtZ0MV_INtOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REBUILD generators explicitly using label_idx and binary mode\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Keep the same augmentations you already used\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    brightness_range=(0.8,1.2),\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Recreate pandas train_df and val_df for the current fold (example uses train_df/val_df variables used in the CV loop)\n",
        "# If you don't have these, please create them from X/ y like in your CV loop:\n",
        "# train_df = pd.DataFrame({'path': X[train_idx], 'label_idx': y[train_idx]})\n",
        "# val_df   = pd.DataFrame({'path': X[val_idx],   'label_idx': y[val_idx]})\n",
        "\n",
        "# Example rebuild (if train_df and val_df exist)\n",
        "train_gen = train_datagen.flow_from_dataframe(train_df, x_col='path', y_col='label_idx',\n",
        "                                              target_size=IMG_SIZE, class_mode='raw', # raw -> returns 0/1 values\n",
        "                                              batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_gen = val_datagen.flow_from_dataframe(val_df, x_col='path', y_col='label_idx',\n",
        "                                          target_size=IMG_SIZE, class_mode='raw',\n",
        "                                          batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Rebuilt generators. train_gen.class_indices:\", getattr(train_gen, 'class_indices', None))\n",
        "if hasattr(train_gen, 'classes'):\n",
        "    import numpy as np\n",
        "    u,c = np.unique(train_gen.classes, return_counts=True)\n",
        "    print(\"train counts:\", dict(zip(u,c)))\n",
        "    u,c = np.unique(val_gen.classes, return_counts=True)\n",
        "    print(\"val counts:\", dict(zip(u,c)))\n"
      ],
      "metadata": {
        "id": "VqXm4mACN4re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUICK SMOKE RUN using the rebuilt generators & binary model\n",
        "model = build_resnet_binary(input_shape=(IMG_SIZE[0],IMG_SIZE[1],3))\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "]\n",
        "\n",
        "steps_per_epoch = min(200, math.ceil(len(train_df) / BATCH_SIZE))  # small for test\n",
        "val_steps = min(80, math.ceil(len(val_df) / BATCH_SIZE))\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=1,\n",
        "    validation_data=val_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Sample predictions after smoke-run\n",
        "val_gen.reset()\n",
        "probs = model.predict(val_gen, steps=val_steps, verbose=1)\n",
        "preds = (probs.ravel() > 0.5).astype(int)\n",
        "# get true labels from val_df for the predicted range\n",
        "true = val_df['label_idx'].values[:len(preds)]\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(true, preds, target_names=['benign','malignant'], zero_division=0))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(true, preds))\n"
      ],
      "metadata": {
        "id": "3yHJF_ACOj2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights dynamically\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y[train_idx]),\n",
        "    y=y[train_idx]\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights)\n"
      ],
      "metadata": {
        "id": "AFe7TwWQZFmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lCuDhYPzNrMf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9f0GOOskugN"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJidgjQUGN7n"
      },
      "outputs": [],
      "source": [
        "# # For each fold:\n",
        "#   • Split data into training and validation subsets while preserving class balance.\n",
        "#   • Build and train a ResNet50 model on the training subset.\n",
        "#   • Evaluate model performance on the validation subset (Accuracy, Precision, Recall, F1).\n",
        "#   • Save best model weights and record per-fold metrics.\n",
        "# Results from all folds are stored in 'fold_metrics' for later summary and analysis.\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold_metrics = []\n",
        "fold_no = 1\n",
        "\n",
        "for train_idx, val_idx in skf.split(X, y):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\" Starting Fold {fold_no}/3 — Train:{len(train_idx)} | Val:{len(val_idx)}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    train_df = pd.DataFrame({'path': X[train_idx], 'label_idx': y[train_idx]})\n",
        "    val_df   = pd.DataFrame({'path': X[val_idx],   'label_idx': y[val_idx]})\n",
        "\n",
        "    classes = np.unique(train_df['label_idx'])\n",
        "    cw = compute_class_weight('balanced', classes=classes, y=train_df['label_idx'])\n",
        "    class_weight = {int(c): w for c,w in zip(classes, cw)}\n",
        "\n",
        "    train_gen = train_datagen.flow_from_dataframe(train_df, x_col='path', y_col='label_idx',\n",
        "                                                  target_size=IMG_SIZE, class_mode='raw',\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_gen = val_datagen.flow_from_dataframe(val_df, x_col='path', y_col='label_idx',\n",
        "                                              target_size=IMG_SIZE, class_mode='raw',\n",
        "                                              batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "   # Build model with binary sigmoid head (single output)\n",
        "   # Use the safe binary model definition to avoid mismatch between labels and outputs.\n",
        "    model = build_resnet_binary(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    ckpt_path = os.path.join(MODELS_DIR, f\"best_resnet_fold{fold_no}.keras\")\n",
        "\n",
        "    csv_log_path = os.path.join(MODELS_DIR, f\"training_log_fold{fold_no}.csv\")\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
        "        ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "        CSVLogger(csv_log_path, append=True)\n",
        "    ]\n",
        "\n",
        "    steps_per_epoch = math.ceil(len(train_df) / BATCH_SIZE)\n",
        "    val_steps = math.ceil(len(val_df) / BATCH_SIZE)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=EPOCHS,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=val_steps,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=callbacks,\n",
        "        verbose=VERBOSE\n",
        "    )\n",
        "\n",
        "\n",
        "    # Evaluate model performance for this fold\n",
        "    val_gen.reset()\n",
        "    # For sigmoid binary output, convert probabilities to class labels with threshold 0.5\n",
        "    preds_prob = model.predict(val_gen, steps=val_steps, verbose=VERBOSE)\n",
        "    # preds_prob shape is (N,1) or (N,) — convert to flat array and threshold\n",
        "    preds = (preds_prob.ravel() > 0.5).astype(int)\n",
        "\n",
        "    true = val_df['label_idx'].values[:len(preds)]\n",
        "\n",
        "    acc = accuracy_score(true, preds)\n",
        "    prec = precision_score(true, preds, zero_division=0)\n",
        "    rec = recall_score(true, preds, zero_division=0)\n",
        "    f1 = f1_score(true, preds, zero_division=0)\n",
        "\n",
        "    print(f\"Fold {fold_no} -> acc:{acc:.4f}, prec:{prec:.4f}, rec:{rec:.4f}, f1:{f1:.4f}\")\n",
        "    print(classification_report(true, preds, target_names=['benign','malignant']))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(true, preds))\n",
        "\n",
        "    fold_metrics.append({'fold':fold_no, 'accuracy':acc,'precision':prec,'recall':rec,'f1':f1})\n",
        "    fold_no += 1\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V_auEesGuXE"
      },
      "outputs": [],
      "source": [
        "# Save the resultes\n",
        "metrics_df = pd.DataFrame(fold_metrics)\n",
        "metrics_csv = os.path.join(MODELS_DIR, \"resnet3fold_metrics.csv\")\n",
        "metrics_df.to_csv(metrics_csv, index=False)\n",
        "print(\"\\nSaved metrics to:\", metrics_csv)\n",
        "print(\"\\nPer-fold metrics:\\n\", metrics_df)\n",
        "print(\"\\nMean metrics:\\n\", metrics_df[['accuracy','precision','recall','f1']].mean())\n",
        "print(\"\\nStd metrics:\\n\", metrics_df[['accuracy','precision','recall','f1']].std())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}